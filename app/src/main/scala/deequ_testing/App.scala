/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package deequ_testing

import com.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame
import com.amazon.deequ.analyzers.runners.{AnalysisRunner, AnalyzerContext}
import com.amazon.deequ.analyzers.{ApproxCountDistinct, Completeness, Size}
import org.apache.spark.sql.SparkSession

object App {
  def main(args: Array[String]): Unit = {
    // Init Spark
    val spark: SparkSession = SparkSession.builder()
      .master("local")
      .appName("exampledeequ")
      .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    import spark.implicits._

    // 1. Load dataset
    val path = "/Applications/XAMPP/xamppfiles/htdocs/deequ_testing/app/src/main/resources/pizzas.csv"
    val dataset = spark.read.option("delimiter", ",").option("header", "true").csv(path)

    dataset.show(false)
    dataset.printSchema()

    // 2. Analysis Metrics
    val analysisResult: AnalyzerContext = {
      AnalysisRunner
        // data to run the analysis on
        .onData(dataset)
        // define analyzers that compute metrics
        .addAnalyzer(Size())
        .addAnalyzer(Completeness("pizza_id"))
        .addAnalyzer(ApproxCountDistinct("pizza_id"))
        // compute metrics
        .run()
    }
    val metrics = successMetricsAsDataFrame(spark, analysisResult);
    metrics.show(truncate = false);


    // 3. Verification
    import com.amazon.deequ.VerificationResult.checkResultsAsDataFrame
    import com.amazon.deequ.checks.{Check, CheckLevel}
    import com.amazon.deequ.{VerificationResult, VerificationSuite}

    val verificationResult: VerificationResult = {
      VerificationSuite()
        // data to run the verification on
        .onData(dataset)
        // define a data quality check
        .addCheck(
          Check(CheckLevel.Error, "Review Check")
            .hasSize(_ >= 90) // at least 90 rows
            .isComplete("pizza_id") // should never be NULL
            .isUnique("pizza_type_id") // should not contain duplicates
        )
        // compute metrics and verify check conditions
        .run()
    }
    // convert check results to a Spark data frame
    val resultDataFrame = checkResultsAsDataFrame(spark, verificationResult)
    resultDataFrame.show(truncate = false);

    // 4. Suggestion Metrics
    import com.amazon.deequ.suggestions.{ConstraintSuggestionRunner, Rules}

    // We ask deequ to compute constraint suggestions for us on the data
    val suggestionResult = {
      ConstraintSuggestionRunner()
        // data to suggest constraints for
        .onData(dataset)
        // default set of rules for constraint suggestion
        .addConstraintRules(Rules.DEFAULT)
        // run data profiling and constraint suggestion
        .run()
    }

    // We can now investigate the constraints that Deequ suggested.
    val suggestionDataFrame = suggestionResult.constraintSuggestions.flatMap {
      case (column, suggestions) =>
        suggestions.map { constraint =>
          (column, constraint.description, constraint.codeForConstraint)
        }
    }.toSeq.toDS()

    suggestionDataFrame.show(truncate = false);
  }
}
